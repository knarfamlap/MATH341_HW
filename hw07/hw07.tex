\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}



\title{MATH 341 / 650.3 Spring 2020 Homework \#7}

\author{Frank Palma Gomez}

\iftoggle{professormode}{
\date{Due by email, Friday 11:59PM, May 8, 2020 \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

 % !TEX root = ./hw07.tex
\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, read about the normal-inverse -gamma / normal-inverse-gamma conjugate model and its marginal posterior distributions and its posterior predictive distribution. Review the general Students $T_\nu$ distribution. Read chapter 16 in the McGrayne book - a very long and important chapter.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to attempt the \textit{difficult} problems. 

Problems marked \qu{[MA]} are for the masters students only (those enrolled in the 650.3 course). For those in 341, doing these questions will count as extra credit.

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}



\problem{These are questions about McGrayne's book, chapter 16.}

\begin{enumerate}
\easysubproblem{What was the main problem facing Bayesian Statistics in the early 1980's?}\spc{4}
\begin{flushleft}
The main problem that Bayesian Statistic was facing in the early 1980's is the \qu{curse of dimensionality.}
\end{flushleft}

\intermediatesubproblem{What is the \qu{curse of dimensionality?}}\spc{4}
\begin{flushleft}
The curese of dimensionality is when data dispersion occurs and dimension increases.
\end{flushleft}
\easysubproblem{How did Bayesian Statistics help sociologists?}\spc{4}
\begin{flushleft}
It helped them by allowing them to test hypothesis accurately given the data within their model.
\end{flushleft}
\easysubproblem{How did Gibbs sampling come to be?}\spc{3}
\begin{flushleft}
Gibbs sampling came to be when the German brothers were trying to reduce the noise and increase the resolution of unfocused images. 
\end{flushleft}

\easysubproblem{Were the Geman brothers the first to discover the Gibbs sampler?}\spc{4}
\begin{flushleft}
No
\end{flushleft}
\easysubproblem{Who officially discovered the expectation-maximization (EM) algorithm? And who \textit{really} discovered it?}\spc{4}
\begin{flushleft}
Aurthur Dempster and Nan Laird first published the algorithm despite Wong discovering it. 
\end{flushleft}
\intermediatesubproblem{How did Bayesians \qu{break} the curse of dimensionality?}\spc{4}
\begin{flushleft}
They broke it by reducing the dimension.
\end{flushleft}
\intermediatesubproblem{Consider the integrals we use in class to find expectations or to approximate PDF's / PMF's --- how can they be replaced?}\spc{4}
\begin{flushleft}
We can use Gibbs sampling.
\end{flushleft}
\easysubproblem{What did physicists call \qu{Markov Chain Monte Carlo} (MCMC)? (p222)}\spc{1}
\begin{flushleft}
They called it statistical sampling.
\end{flushleft}
\easysubproblem{Why is sampling called \qu{Monte Carlo} and who named it that?}\spc{4}
\begin{flushleft}
Named after Stanislaw Ulams gambling uncle by Nicolas Metropolis
\end{flushleft}
\easysubproblem{The Metropolis-Hastings (MH) Algorithm is world famous and used in myriad applications. Why didn't Hastings get any credit?}\spc{4}
\begin{flushleft}
When Hasting published the paper, powerful computers were not around.
\end{flushleft}

\easysubproblem{The combination of Bayesian Statistics + MCMC has been called ... (p224)}\spc{1}
\begin{flushleft}
"Most powerful mechanism ever created for processing data and knowledge."
\end{flushleft}
\extracreditsubproblem{p225 talks about Thomas Kuhn's ideas of \qu{paradigm shifts.} What is a \qu{paradigm shift} and does Bayesian Statistics + MCMC qualify?}\spc{8}


\easysubproblem{How did the \href{http://www.mrc-bsu.cam.ac.uk/software/bugs/}{BUGS} software change the world?}\spc{4}
\begin{flushleft}
Made MCMC available to everyone.
\end{flushleft}
\easysubproblem{Lindley said that Bayesian Statistics would win out over Frequentist Statistics because it was more logical. What in reality was the reason for the eventual victory of Bayes?}\spc{4}
\begin{flushleft}
It gained influence from multiple desciplines that used it.
\end{flushleft}
\extracreditsubproblem{One of my PhD advisors, \href{https://statistics.wharton.upenn.edu/profile/563/}{Ed George} at Wharton told me that \qu{Bayesian Statistics is really `knowledge engineering.'} Is this true? Explain.}\spc{3}

\extracreditsubproblem{Take a look at the software \href{http://mc-stan.org/}{Stan}. What kind of potential does it have to change the world? Note: I had an opportunity to work on Stan as a postdoc (right after I finished his PhD) but chose to come to QC instead.}\spc{10}

\end{enumerate}




\input{R_equations_table}



\problem{Now we will move to the Bayesian normal-normal model for estimating both the mean and variance and demonstrate similarities with the classical results.}

\begin{enumerate}

\intermediatesubproblem{If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and $X$ represents all $\Xoneton$, Find the kernel of $\cprob{\theta,~\sigsq}{X}$ if $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. Use the substitution that we made in class:

\beqn
\sum_{i=1}^n (x_i - \theta)^2 = (n-1)s^2 + n(\xbar - \theta)^2
\eeqn

where $s^2 := \oneover{n-1} \sum_{i=1}^n (x_i -\xbar)^2$. We do this here because this substitution is important for what comes next.}\spc{8}
\begin{align*}
    \cprob{\theta,~\sigsq}{X} 
    &\propto \cprob{X}{\theta,~\sigsq} \prob{\theta,~\sigsq}\\
    &\propto (2\pi\sigsq)^{-\frac{n}{2}}\exp{-\frac{1}{2\sigsq}\sum_{i=0}^{n}(x_i-\theta)^2}(\frac{1}{\sigsq})\\
    &\propto (\sigsq)^{-\frac{n}{2}-1} \exp{-\frac{(n-1)s^2/2}{\sigsq}} \exp{-\frac{n}{2\sigsq}(\xbar-\theta)^2}\\
    &\propto \text{NormInvGamma}(\xbar, ~n, ~\frac{n}{2}, ~\frac{(n-1)s^2}{2}) 
\end{align*}

\intermediatesubproblem{Using Bayes Rule, break up $\cprob{\theta,~\sigsq}{X}$ into two pieces. How are those two pieces distributed?}\spc{6}
\begin{align*}
   \text{Recall: } \cprob{\theta,~\sigsq}{X}  &\propto \cprob{X}{\theta,~\sigsq} \cprob{~\sigsq}{X}\\
   \cprob{X}{\theta,~\sigsq}  &\propto  \normnot{\xbar}{\frac{\sigsq}{n}} \\
    \cprob{~\sigsq}{X} &\propto \text{InvGamma}(\frac{n-1}{2},\frac{(n-1)s^2}{2})
\end{align*}

\intermediatesubproblem{Using your answer from (b), explain in English how you can create samples from the distribution $\cprob{\theta,~\sigsq}{X}$ that look like $\braces{\bracks{\theta_1, \sigsq_1}, \bracks{\theta_2, \sigsq_2}, \ldots, \bracks{\theta_S, \sigsq_S}}$.}\spc{8}
\begin{enumerate}
    \item Sample $\sigsq_{samp}$ from $\cprob{~\sigsq}{X}$
    \item Sample $\theta_{samp}$ from $\normnot{\xbar}{\frac{\sigsq}{n}}$
    \item Return $[\theta_{samp}, \sigsq_{samp}]$
    \item Repeat previous steps s times
\end{enumerate}

\hardsubproblem{Using these samples, how would you estimate $\cexpe{\theta}{X}$ and $\cexpe{\sigsq}{X}$? Why is $\cexpe{\theta}{X}$ of paramount importance?}\spc{5}

\begin{enumerate}
    \item $\cexpe{\theta}{X} \approx \frac{\theta_{samp, 1} +\cdots + \theta_{samp, s}}{s}$
    \item $\cexpe{\sigsq}{X} \approx \frac{\sigsq_{samp, 1} +\cdots+\sigsq_{samp, s }}{s}$
\end{enumerate}

Because $\cexpe{\theta}{X}$ is the best guess for $\theta$.

\hardsubproblem{Using these samples, how would you estimate a 95\% CR for $\theta$?}\spc{5}
\begin{align*}
    CR_{\theta, 95\%} = [\texttt{samplequantile}(\theta_j, 0.025), \texttt{samplequantile}(\theta_j, 0.975)]
\end{align*}

\hardsubproblem{Using these samples, how would you obtain a $p$-val for testing if $\sigsq > 1.364$?}\spc{5}

\begin{align*}
     p_{val} = \frac{1}{N} \sum_{l = 1} \mathds{1} \sigsq_{l,j} \leq 1.364
\end{align*}

\hardsubproblem{[MA] Using these samples, how would you estimate $\corr{\theta~|~X}{~\sigsq~|~X}$ i.e. the correlation between the posterior distributions of the two parameters?}\spc{3}

%\easysubproblem{If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and $\theta \sim \normnot{\mu_0}{\tausq}$ write the distribution of $\theta~|~X,\sigsq$. Hint: it's in the notes and it was HW6 6(d). Note this problem is independent of the other problems.}\spc{5}

\easysubproblem{Find $\cprob{\theta}{X,~\sigsq}$ by using the full posterior kernel from (a) and then conditioning on $\sigsq$. You should get the same answer as we did before the midterm.}\spc{3}

\begin{align*}
    \cprob{\theta}{X, ~\sigsq} = \normnot{\xbar}{\frac{\sigsq}{n}}
\end{align*}

\easysubproblem{Find $\cprob{\sigsq}{X,~\theta}$ by using the full posterior kernel from (a)  and then conditioning on $\theta$. You should get the same answer as we did before the midterm.}\spc{4}

\begin{align*}
    \cprob{\sigsq}{X, ~\theta} = \invgammanot{\frac{n-n_0}{2}}{\frac{n\sigsq_{MLE} + n_0\sigsq_{0}}{2}}
\end{align*}

%\intermediatesubproblem{Show that $\cprob{\sigsq}{X}$ is an inverse gamma distribution and find its parameters.}\spc{4}

\hardsubproblem{Show that $\cprob{\theta}{X}$ is a non-standard $T$ distribution and find its parameters. Assume the prior $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. The answer is in the notes, but try to do it yourself.}\spc{15}
\begin{align*}
    \cprob{\theta}{X} &= \int^{\infty}_0 \cprob{\theta, ~\sigsq}{X} d\sigsq \\
    &\propto \int^{\infty}_0 (\sigsq)^{-\frac{n}{2}-1} \exp{-\frac{(n-1)s^2/2}{\sigsq}} \exp{-\frac{n}{2\sigsq}(\theta-\xbar)^2} d\sigsq\\
    &\propto \Gamma\Big(\frac{n}{2}\Big)\Big(\frac{(n-1)s^2 + n(\theta-\xbar)^2}{2}\Big)^{-\frac{n}{2}}\\
    &\propto \Big(\frac{(n-1)s^2 + n(\theta-\xbar)^2}{2}\Big)^{-\frac{n}{2}}\\
    &\propto \Big(\frac{(n-1)s^2}{2}\Big)^{-\frac{n}{2}} \Big(1 + \frac{ n(\theta-\xbar)^2}{(n-1) s^2}\Big)^{\frac{-(n-1)-1}{2}}\\
    &\propto \Bigg( 1 + \frac{1}{n-1} \Big( \frac{(\theta-\xbar)^2}{s^2/2} \Big)\Bigg)^{\frac{-(n-1)-1}{2}}\\
    &\propto T_{n-1}\Big( \xbar, \frac{s}{\sqrt{n}}\Big)
\end{align*}

\hardsubproblem{Show that $\cprob{\sigsq}{X}$ is an inverse gamma and find its parameters. Assume the prior $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. The answer is in the notes, but try to do it yourself.}\spc{12}


%\intermediatesubproblem{How does this compare to 2(j)? Note that $X \sim \invgammanot{\alpha}{\beta}$ then $cX \sim \invgammanot{\alpha}{\frac{\beta}{c}}$.}\spc{2}

\easysubproblem{Write down the distribution of $\cprob{X^*}{X}$ assuming the prior $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. This is in the notes.}\spc{1}

\begin{align*}
    T_{n-1}(\xbar, s~\sqrt{\frac{n+1}{n}}) 
\end{align*}

\extracreditsubproblem{Prove what you wrote in the previous question: $\cprob{X^*}{X}$ is the non-standard $T$ distribution and find its parameters.}\spc{0}


\intermediatesubproblem{Explain how to sample from the distribution of $\cprob{X^*}{X}$. Also in the notes.}\spc{9}
\begin{enumerate}
    \item Sample $\sigsq_{samp}$ from $\invgammanot{\frac{n-1}{2}}{\frac{(n-1)s^2}{2}}$
    \item Sample $\theta_{samp}$ from  $\normnot{\xbar}{\frac{\sigsq}{n}}$ where $\theta=\theta_{samp}$
    \item Sample $X^*_{samp}$ from $\normnot{\theta}{\sigsq}$ where $\theta=\theta_{samp}$ and $\sigsq=\sigsq_{samp}$
    \item Return $X^*_{samp}$ 
    \item Repeat previous steps s times
\end{enumerate}

\intermediatesubproblem{Now consider the informative conjugate prior of $\prob{\theta,~\sigsq} = \cprob{\theta}{\sigsq} \prob{\sigsq}$ where $\cprob{\theta}{\sigsq} = \normnot{\mu_0}{\frac{\sigsq}{m}}$ and $\prob{\sigsq} = \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$ i.e. the general normal-inverse-gamma. What is its kernel? Collect common terms and be neat.}\spc{9}
\begin{align*}
    \prob{\theta,~\sigsq} 
    &= \normnot{\mu_0}{\frac{\sigsq}{m}}\invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}\\
    &\propto \exp{\frac{\mu_0 m}{\sigsq}\theta - \frac{m}{2\sigsq}\theta^2(\sigsq)^{-\frac{n_0}{2}-1}} \exp{-\frac{n_0\sigsq_0}{2\sigsq}}\\
    &\propto \exp{\frac{\mu_0 m}{\sigsq}\theta -\frac{m}{2\sigsq}\theta^2 -\frac{n_0\sigsq_0}{2\sigsq}}(\sigsq)^{-\frac{n_0}{2}-1}
\end{align*}

\hardsubproblem{[MA] If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and given the general prior above, find the posterior and demonstrate it that the normal-inverse gamma is conjugate for the normal likelihood with both mean and variance unknown. This is what I did \emph{not} do in class.}\spc{13}

\end{enumerate}

\problem{We model the returns of S\&P 500 here.}

\begin{enumerate}
\easysubproblem{Below are the 16,428 daily returns (as a percentage) of the S\&P 500 dating back to January 4, 1950 and the code used to generate it. Does the data look normal? Yes/no}\spc{0}
Yes.

\begin{figure}[h]
\centering
\includegraphics[width=7in]{daily_returns}
\end{figure}

%\begin{verbatim}
%X = read.csv('sp_tot_ret_price_1950.csv')
%n = nrow(X)
%n
%hist(X[,4], br = 1000, 
%  main = 'daily returns (as a percentage) of the S&P 500')
%\end{verbatim}

\intermediatesubproblem{Do you think the data is $\iid$? Explain.}\spc{1}
No because previous days can influence the days in the future. 

\intermediatesubproblem{Assume $\iid$ normal data regardless of what you wrote in (a) and (b). The sample average is $\xbar = 0.0003415$ and the sample standard deviation is $s = 0.0096$. Under an objective prior, give a 95\% credible region for the true mean daily return.}\spc{4}
\begin{align*}
    CR_{\theta,95\%} &= [\texttt{qnorm}(0.025, 0.0003415, \frac{0.0096^2}{16428}), 
        \texttt{qnorm}(0.975, 0.003415, \frac{0.0096^2}{16428})]\\
    & =[0.00341489, 0.00341511]
\end{align*}

\hardsubproblem{Give a 95\% credible region for \emph{tomorrow's} return using functions in Table~\ref{tab:eqs}.}\spc{4}
\begin{align*}
    CR_{95\%} &= [\texttt{qnorm}(0.025,  0.003415, \squared{0.0096}), \texttt{qnorm}(0.975, 0.003415,\squared{0.0096})]\\
    & =[0.0001608697, 0.0005221303]
\end{align*}

\end{enumerate}


\end{document}



\problem{This problem is about the normal-normal model using a \qu{semi-conjugate} prior. Assume $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ throughout.}

\begin{enumerate}

\easysubproblem{If $\theta$ and $\sigsq$ are assumed to be independent, how can $\prob{\theta,~\sigsq}$ be factored?}\spc{1}

\easysubproblem{If $\prob{\theta} = \normnot{\mu_0}{\tausq}$ and $\prob{\sigsq} \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$, find the kernel of the joint posterior, $\cprob{\theta,~\sigsq}{X}$}.\spc{4}

\hardsubproblem{Show that this kernel can be factored into the kernel of a normal where the leftover is \textit{not} the kernel of an inverse gamma. This is in the lecture notes.}\spc{12}

\hardsubproblem{[MA] Find the posterior mode of $\sigsq$ using $k(\sigsq~|~X)$.}\spc{5}

\hardsubproblem{Describe how you would sample from $k(\sigsq~|~X)$. Make all steps explicit and use the notation from Table~\ref{tab:eqs}.}\spc{10}

\hardsubproblem{Describe how you would sample from $\cprob{\theta,~\sigsq}{X}$. Make use of the sampling algorithm in the previous question. Make all steps explicit and use the notation from Table~\ref{tab:eqs}.}\spc{10}
 


\hardsubproblem{What are the two main disadvantages of grid sampling?}\spc{4}

\hardsubproblem{Why do you think the prior $\prob{\theta} = \normnot{\mu_0}{\tausq}$ and $\prob{\sigsq} \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$ is called \qu{semi-conjugate}?}\spc{4}


\extracreditsubproblem{[MA] Find the MMSE of $\sigsq$}\spc{0}

\end{enumerate}



\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\problem{These are questions about McGrayne's book, chapter 17 and the Epilogue.}

\begin{enumerate}

\easysubproblem{What do the computer scientists who adopted Bayesian methods care most about and whose view do they subscribe to? (p233)}\spc{1}

\easysubproblem{How was \qu{Stanley} able to cross the Nevada desert?}\spc{3}

\easysubproblem{What two factors are leading to the \qu{crumbling of the Tower of Babel?}}\spc{3}

\intermediatesubproblem{Does the brain work through iterative Bayesian modeling?}\spc{4}

\easysubproblem{According to Geman, what is the most powerful argument for Bayesian Statistics?}\spc{3}

\end{enumerate}







