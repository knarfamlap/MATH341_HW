\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
% \toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 341 / 650.3 Spring 2020 Homework \#8}

\author{} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due by email, Tuesday 11:59PM, May 19, 2020 \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

% !TEX root = ./hw08.tex
\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, read about the semi-conjugate normal-inverse-gamma model, grid sampling, Gibbs sampling, and Bayes Factors. Finish the McGrayne book.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to attempt the \textit{difficult} problems. 

Problems marked \qu{[MA]} are for the masters students only (those enrolled in the 650.3 course). For those in 341, doing these questions will count as extra credit.

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}


\problem{These are questions about McGrayne's book, chapter 17 and the Epilogue.}

\begin{enumerate}

\easysubproblem{What do the computer scientists who adopted Bayesian methods care most about and whose view do they subscribe to? (p233)}
\begin{flushleft}
    Computer scientist cared only about results and now they subscribe to John Tukey's view.
\end{flushleft}

\easysubproblem{How was \qu{Stanley} able to cross the Nevada desert?}
\begin{flushleft}
    Stanley was moving in an average speed, its camera took images of the route and the computer estimated the probability of various obstacles. 
    As the robot navigated sharp turns and cliffs and generally stayed on course, and it slowed down to avoid even unlikely catastrophes.
\end{flushleft}

\easysubproblem{What two factors are leading to the \qu{crumbling of the Tower of Babel?}}
\begin{itemize}
    \item The ability to accumulate evidence is an optimal survival strategy.
    \item Dependence on Bayes’ theorem for calculating each individual has mastered a topic and is ready for a new challenge.
\end{itemize}

\intermediatesubproblem{Does the brain work through iterative Bayesian modeling?}
\begin{flushleft}
    Yes.
\end{flushleft}

\easysubproblem{According to Geman, what is the most powerful argument for Bayesian Statistics?}
\begin{flushleft}
    "...there is no more powerful argument for Bayes than its recognition of the brain’s inner structures and prior expectations.”
\end{flushleft}

\end{enumerate}


\input{R_equations_table}

\problem{This problem is about the normal-normal model using a \qu{semi-conjugate} prior. Assume $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ throughout.}

\begin{enumerate}

\easysubproblem{If $\theta$ and $\sigsq$ are assumed to be independent, how can $\prob{\theta,~\sigsq}$ be factored?}

\[
    \prob{\theta,~\sigsq} = \prob{\theta}\prob{~\sigsq} 
\]

\easysubproblem{If $\prob{\theta} = \normnot{\mu_0}{\tausq}$ and $\prob{\sigsq} \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$, find the kernel of the joint posterior, $\cprob{\theta,~\sigsq}{X}$}.

\begin{align*}
    \cprob{\theta,~\sigsq}{X} 
    &= \cprob{X}{\theta,~\sigsq}\prob{\theta}\prob{~\sigsq}\\
    &= \Pi \normpdf{X_i}{\theta}{\sigsq} \exp{\frac{1}{2\tausq}(\theta - \mu_0)^2}\\
    &\propto (\sigsq)^{-\frac{n}{2}} 
    e^{-\frac{1}{2\sigsq}\big((n-1)s^2 + n(\xbar-\theta)^2\big)} e^{-\frac{1}{2\tausq}(\theta-\mu_0)^2}
    (\sigsq)^{-\frac{n_0}{2}-1} 
\end{align*}

\hardsubproblem{Show that this kernel can be factored into the kernel of a normal where the leftover is \textit{not} the kernel of an inverse gamma. This is in the lecture notes.}\spc{12}

\begin{align*}
    (\sigsq)^{-\frac{n}{2}} &
    e^{-\frac{1}{2\sigsq}\big((n-1)s^2 + n(\xbar-\theta)^2\big)} e^{-\frac{1}{2\tausq}(\theta-\mu_0)^2}
    (\sigsq)^{-\frac{n_0}{2}-1}\\
    &\propto (\sigsq)^{-\frac{n}{2}-\frac{n_0}{2}-1} 
    e^{-\frac{1}{2\sigsq}\big((n-1)s^2 + n_0\sigsq_0 + n\xbar^2\big)}
    e^{\big(\frac{n\xbar}{\sigsq} + \frac{\mu_0}{\tausq}\big)\theta - \big( \frac{n}{2\sigsq} + \frac{1}{\tausq}\big)\theta^2}\\
    &= (\sigsq)^{-\frac{n}{2}-\frac{n_0}{2}-1} 
    e^{-\frac{1}{2\sigsq}\big((n-1)s^2 + n_0\sigsq_0 + n\xbar^2\big)}
    \sqrt{\frac{\pi}{b}}e^{-\frac{a^2}{4b}} \normnot{\frac{a}{2b}}{\frac{1}{2b}}
    \cprob{X,~\sigsq}{\theta} \\
    &\propto (\sigsq)^{-\frac{n}{2}-\frac{n_0}{2}-1} 
    e^{-\frac{1}{2\sigsq}\big((n-1)s^2 + n_0\sigsq_0 + n\xbar^2\big)}\Big( \frac{n}{2\sigsq} + \frac{1}{\pi}\Big)^{-\frac{1}{2}}
    e^{\big(\frac{n\xbar}{\sigsq} + \frac{\mu_0}{\tausq}\big)^2 / 4\big( \frac{n}{2\sigsq} + \frac{1}{2\tausq}\big)}\\
\end{align*}

\hardsubproblem{[MA] Find the posterior mode of $\sigsq$ using $k(\sigsq~|~X)$.}\spc{5}

\hardsubproblem{Describe how you would sample from $k(\sigsq~|~X)$. Make all steps explicit and use the notation from Table~\ref{tab:eqs}.}

\begin{enumerate}
    \item Select $\sigsq_{min}, \sigsq_{max} \Delta$ for: $<\sigsq_{min}, \sigsq_{min} + \Delta, \cdots, \sigsq_{max}>$ 
    \item Compute $C \approx \frac{1}{\sum_{\sigsq \in G} k(\sigsq~|~X)}$ for all $\sigsq_0 \in G$ compute $F(\sigsq_0~|~X) = \prob{\sigsq \leq \sigsq_0~|~X}$
    \item Draw $u$ from runif(0,1) and ship $X_{samp} = avgmin_{\sigsq \in G} \{ F(\sigsq~|~X) \geq u \}$
    \item Repeat c
\end{enumerate}


\hardsubproblem{Describe how you would sample from $\cprob{\theta,~\sigsq}{X}$. Make use of the sampling algorithm in the previous question. Make all steps explicit and use the notation from Table~\ref{tab:eqs}.}
 
\begin{enumerate}
    \item Draw $\sigsq_{samp}$ from $k(\sigsq~|~X)$
    \item Draw $\theta_{samp}$ from $\normnot{\theta}{\sigsq}$
    \item Ship $<\theta_{samp}, \sigsq_{samp}>$ as one sample
    \item Repeat a-c s times
\end{enumerate}


\hardsubproblem{What are the two main disadvantages of grid sampling?}

\begin{enumerate}
    \item Selecting the min, max and $\Delta$ are needed.
    \item if $\Delta$ is small and $dim[\theta]$ is large, grid sampling fails.
\end{enumerate}

\hardsubproblem{Why do you think the prior $\prob{\theta} = \normnot{\mu_0}{\tausq}$ and $\prob{\sigsq} \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$ is called \qu{semi-conjugate}?}

It combines the normal and the inverse gamma to make a normal inverse gamma distribution.

\extracreditsubproblem{[MA] Find the MMSE of $\sigsq$}\spc{0}

\end{enumerate}

\problem{These are questions which introduce Gibbs Sampling.}

\begin{enumerate}
\easysubproblem{Outline the systematic sweep Gibbs Sampler algorithm below (in your notes).}

\begin{enumerate}
    \item Begin with a reasonable value for  $\theta(\theta_0=\theta)$
    \item Draw $\sigsq_1$ from $\cprob{\sigsq}{X,\theta}$ where $\theta_0=\theta$
    \item Draw $\theta_1$ from $\cprob{\theta}{X,\sigsq}$ where $\sigsq=\sigsq_1$
    \item Draw $\sigsq_2$ from $\cprob{\sigsq}{X,\theta}$ where $\theta=\theta_1$
    \item Draw $\theta_2$ from $\cprob{\theta}{X,\sigsq}$ where $\sigsq=\sigsq_2$
    \item Repeat s times
\end{enumerate}

\extracreditsubproblem{Under what conditions does this algorithm converge?}\spc{0}

\easysubproblem{Pretend you are estimating $\cprob{\theta_1,~\theta_2}{X}$ and the joint posterior looks like the picture below where the $x$ axis is $\theta_1$ and the $y$ axis is $\theta_2$ and darker colors indicate higher probability. Begin at $\bracks{\theta_1,\theta_2} = \bracks{0.5,0.5}$ and simulate 5 iterations of the systematic sweep Gibbs sampling algorithm by drawing new points on the plot (just as we did in class).}


\begin{figure}[htp]
\centering
\includegraphics[width=3.5in]{contour.png}
\end{figure}
\end{enumerate}


\problem{These are questions about the change point model and the Gibbs sampler to draw inference for its parameters. You will have to use R to do this question. If you do not have it installed on your computer, you can use R online without installing anything by using a site like \href{https://hub.gke.mybinder.org/user/binder-examples-r-vegj1tni/notebooks/Untitled1.ipynb?kernel_name=ir}{jupyter}. You copy code into the black box and click the \qu{run} button atop. Then you enter more code into the next box and click \qu{run} again, etc.}

\begin{enumerate}

\easysubproblem{Consider the change point Poisson model we looked at in class. We have $m$ exchangeable Poisson r.v.'s with parameter $\lambda_1$ followed by $n-m$ exchangeable Poisson r.v.'s with parameter $\lambda_2$. Both rate parameters and the value of $m$ are unknown so the parameter space is 3-dimensional. Write the likelihood below.}
\begin{align*}
    \cprob{\lambda_1, \lambda_2, m}{X_1, \cdots, X_n}
    &\propto \cprob{X_1, \cdots, X_n}{\lambda_1, \lambda_2, m}\prob{lambda_1, \lambda_2, m}\\
    &= \Bigg( \prod^m_{t=1} \frac{e^{-\lambda_1}\lambda_1^{X_t}}{X_t!}\Bigg)
    \Bigg( \prod^n_{t=m+1} \frac{e^{-\lambda_2}\lambda_2^{X_t}}{X_t!}\Bigg)
    \Bigg( \frac{1}{n-1}\Bigg)
\end{align*}

\easysubproblem{Consider the model in (a) where $\lambda_1 = 2$ and $\lambda_2 = 4$ and $m=10$ and $n=30$. Run the code on lines 1--14 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here} by copying them from the website and pasting them into an R console. This will plot a realization of the data with those parameters. Can you identify the change point visually?}
\begin{flushleft}
    The change point is approximately 12.
\end{flushleft}


\easysubproblem{Consider the model in (a) but we are blinded to the true values of the parameters given in (b) and we wish to estimate them via a Gibbs sampler. Run the code on lines 16--78 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here} which will run 10,000 iterations. What iteration number do you think the sampler converged?}

\begin{flushleft}
    It looks like 10.   
\end{flushleft}

\easysubproblem{Now we wish to assess autocorrelation among the chains from the Gibbs sampler run in (d). Run the code on lines 79--89 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here}. What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{1}
\begin{flushleft}
    8
\end{flushleft}

\easysubproblem{Run the code on lines 91--121 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec20_demos/poisson_gamma_change_pt.R}{here} which will first burn and thin the chains. Explain these three plots. What distributions do these frequency histograms approximate? You must have $\prob{\text{something}}$ in your answer. What are the blue lines? What are the red lines? What are the grey lines? Read the code if you have to for the answers.}
\begin{itemize}
    \item The histograms approximate $\cprob{\lambda_1}{X_1, \cdots, X_n, \lambda_2, m}, \cprob{\lambda_2}{X_1, \cdots, X_n, \lambda_1, m}$ and $\cprob{m}{X_1, \cdots, X_n, \lambda_1, \lambda_2}$.
    \item The blue line are the means $(\lambda_1 =1.6,\lambda_2 =3.25, m = 9.4)$.
    \item The red lines are the true values $(\lambda_1 =2,\lambda_2 =4, m = 10)$.
    \item The grey lines are the $95\%$ credible regions $(\lambda_1:[0.7,2.7],\lambda_2:[2.4,4.2], m:[2,19])$.
\end{itemize}

\hardsubproblem{Test the following hypothesis: $H_0: m \leq 15$ by approximating the $p$-value from one of the plots in (e).}
\begin{align*}
    P_{val} &= \cprob{m\leq 15}{X}\\
    &= \frac{1}{30} \Sigma \mathds{1} m \leq 15 \\
    &= 0.9597598 > (\alpha = 0.05)
\end{align*}

\hardsubproblem{[M.A.] Explain a procedure to test $H_0: \lambda_1 = \lambda_2$. You can use the plots if you wish, but you do not have to.}

\hardsubproblem{What exactly would come from $\cprob{X^*}{X}$ in the context of this problem? Assume $X^*$ is the same dimension of $X$ (in our toy example, $n=30$). Explain in full detail. Be careful!}
\begin{flushleft}
    It would predict the 31st data point given our estimates ($\lambda_1, \lambda_2, m$) and the previous data.
    $\cprob{X^*}{X} = \int \int \int \cprob{X^*}{\lambda_1, \lambda_2, m} \cprob{\lambda_1, \lambda_2, m}{X} d\lambda_1 d\lambda_2 dm$
\end{flushleft}

\extracreditsubproblem{Explain how you would estimate $\cov{\lambda_1}{\lambda_2}$ and what do you think this estimate will be close to?}\spc{7}


\end{enumerate}


\problem{These are questions about the mixture-of-two-normals model and the Gibbs sampler to draw inference for its parameters. You will have to use R to do this question. If you do not have it installed on your computer you can use \href{http://rextester.com/l/r_online_compiler}{this website} which will give you provide you with a workable R console.}

\begin{enumerate}


\easysubproblem{Consider the mixture-of-two-normals model we looked at in class. Write the likelihood below.}\spc{4}

\begin{align*}
    \prob{\theta_1, \sigsq_1, \theta_2, \sigsq_2, ~\rho} &= \prod_{i=1}^{n} (\rho \frac{1}{\sqrt{2\pi\sigsq_1}}\exp{-\frac{1}{2\sigsq_1} \squared{x_i-\theta_1}} + (1 - \rho) \frac{1}{\sqrt{2\pi\sigsq_2}}\exp{-\frac{1}{2\sigsq_2}\squared{x_i-\theta_2}})
\end{align*}

\easysubproblem{Consider the model in (a) with $\theta_1 = 0,~\theta_2 =4,~\sigsq_1 = 2,~\sigsq_2 = 1$ and $\rho = 2$. These are the targets of inference so pretend you don't know their values! Run the code on lines 1--16 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here} by copying them from the website and pasting them into an R console. This will plot a realization of the data with those parameters. Can you identify that it's a mixture of two normals visually?}\spc{1}

   Yes since there are two humps. Some will samples belong to the first and some to the second one. 

\easysubproblem{Consider the model in (a) but we are blinded to the true values of the parameters given in (b) and we wish to estimate them via a Gibbs sampler. Run the code on lines 19--92 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here} which will run 10,000 iterations. What iteration number do you think the sampler converged?}\spc{1}

    Iteration number of about 50

\easysubproblem{Now we wish to assess autocorrelation among the chains from the Gibbs sampler run in (d). Run the code on lines 96--103 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here}. What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{3}


    We can modify the thinning, include more samples, or more iterations. 

\easysubproblem{Run the code on lines 120--152 of the code at the link \href{https://github.com/kapelner/QC_Math_341_Spring_2017/blob/master/lectures/lec21_demos/mixture_model/mixture_model_gibbs.R}{here} which will first burn and thin the chains. Explain these five plots. What distributions do these frequency histograms approximate?}\spc{4}

    The blue line represents the best guess for each of the five parameters. These distograms approximate the normal distribution. 

\hardsubproblem{Provide and approximate $CR_{\rho, 95\%}$. Does it capture the true value of $\rho$?}\spc{4}

    \begin{align*}
        \text{CR}_{\rho, 95\%} &= [0.235, 0.375] \text{, It does not capture $\rho$}
    \end{align*}

\hardsubproblem{Explain carefully how you would approximate $\cprob{X^*}{X}$.}\spc{4}


\hardsubproblem{Explain carefully how you would approximate the probability that the 17th observation belonged to the $\normnot{\theta_1}{\sigsq_1}$ distribution.}\spc{4}

    Take th expectation of the best guess. With that you can see the probability if the 17th observation belongs to $\normnot{\theta_1}{\sigsq_1}$

\easysubproblem{If one of the $\theta$'s did not have a known conditional distribution, which algorithm could you use? Would this algorithm take longer or shorter to converge than the Gibbs sampler you've seen here?}\spc{3}

    You could use grid sampling. This algorithm would take longer to converge. 

\extracreditsubproblem{Explain carefully how you would test if $\theta_1 \neq \theta_2$.}\spc{4}

\end{enumerate}

\problem{This question is about a famous  extra sensory perception (ESP) experiment.}

\begin{figure}[htp]
\centering
\includegraphics[width=2.8in]{esp.jpg}
\end{figure}

According to quantum mechanics, an event should happen with exactly probability 50\%. So if someone comes with a claim that they have ESP and can affect this event with their mind, then the event no longer has probability 50\% (this is what we are trying to prove).

An experiment was run with someone claiming they have ESP. They tried to affect the event with their mind. The data is as follows: of $n= 104,490,000$ observations, the number of events was $x = 52,263,970$. We will now test to see if the person has ESP a number of ways and try to reconcile the differences.

\begin{enumerate}

\easysubproblem{What is the MLE for the probability of the event?}\spc{1}

\begin{align*}
    \thetahatmmse = \xbar = \frac{1}{n} \sum x_i = 0.5001
\end{align*}

\easysubproblem{Run a two-sided frequentist test at $\alpha = 5\%$ and report the decision (i.e. retain or reject) and the $p$ value.}\spc{2}

\begin{align*}
    \text{RR} &= [0.5 ~\pm~ 2\sqrt{\frac{0.5(1-0.5)}{104, 490, 000}}] = [0.49, 0.50] \\ 
    \text{H}_0 &: \theta = 0.5 \\ 
    \text{H}_a &: \theta \neq 0.5 \\ 
    \theta &\in \text{RR}_{\theta, 95\%} \text{ Retain $H_0$ } \\ 
    P_{val} &= 0.003 < \alpha = 0.5
\end{align*}

\intermediatesubproblem{Run a two-sided Bayesian test assuming the principle of indifference. Use the equivalence region approach with $\delta = 0.01$. Report the decision (i.e. retain or reject) and the $p$ value.}\spc{3}

\begin{align*}
    \text{H}_0 &: \theta = 0.5 \\ 
    \text{H}_a &: \theta \neq 0.5 \\ 
   \text{CR}_{\theta, 95\%} &= [\text{qbeta}(0.025, x + 1, n - x + 1), \text{qbeta}(0.975, x+1, n-x+1)] \\ 
    &= [0.50008567, 0.5002774]
\end{align*}

Ratain $H_0$ since $\theta \in $ CR$_{\theta, 95\%}$ 

\hardsubproblem{Calculate the Bayes Factor $B$ where the alternative hypothesis of $\theta \neq 0.5$ can be summed up with $\theta \sim \stduniform$.}\spc{6}

\begin{align*}
    \text{H}_0 &: \theta = 0.5 \\ 
    \text{H}_a &: \theta \neq 0.5 \\ 
    \alpha &= 5\% \\ 
    \text{B} &= \frac{\text{B}(52,263,971, ~52,226,031)}{(0.5)^{104, 490, 000}} \\
    &\approx 0.33
    % \text{B} &= \text{B}
\end{align*}

Retain $H_0$ 



\hardsubproblem{Try to reconcile parts (b), (c), (d), (e). Why are there different answers? What is going on??}\spc{6}

    With the frequentist pval, bayesian pval, and Bayes Factor B we access "statistical significance". We could also messure "clinical significance", how important the actual effect of theta is. 
    Bayes factors puts both of these ideas together. 

\end{enumerate}

\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}


\problem{These are questions about McGrayne's book, chapter 16.}

\begin{enumerate}
\easysubproblem{What was the main problem facing Bayesian Statistics in the early 1980's?}\spc{4}

\intermediatesubproblem{What is the \qu{curse of dimensionality?}}\spc{4}

\easysubproblem{How did Bayesian Statistics help sociologists?}\spc{4}

\easysubproblem{How did Gibbs sampling come to be?}\spc{3}

\easysubproblem{Were the Geman brothers the first to discover the Gibbs sampler?}\spc{4}

\easysubproblem{Who officially discovered the expectation-maximization (EM) algorithm? And who \textit{really} discovered it?}\spc{4}

\intermediatesubproblem{How did Bayesians \qu{break} the curse of dimensionality?}\spc{4}

\intermediatesubproblem{Consider the integrals we use in class to find expectations or to approximate PDF's / PMF's --- how can they be replaced?}\spc{4}

\easysubproblem{What did physicists call \qu{Markov Chain Monte Carlo} (MCMC)? (p222)}\spc{1}

\easysubproblem{Why is sampling called \qu{Monte Carlo} and who named it that?}\spc{4}

\easysubproblem{The Metropolis-Hastings (MH) Algorithm is world famous and used in myriad applications. Why didn't Hastings get any credit?}\spc{4}

\easysubproblem{The combination of Bayesian Statistics + MCMC has been called ... (p224)}\spc{1}


\extracreditsubproblem{p225 talks about Thomas Kuhn's ideas of \qu{paradigm shifts.} What is a \qu{paradigm shift} and does Bayesian Statistics + MCMC qualify?}\spc{8}

\easysubproblem{How did the \href{http://www.mrc-bsu.cam.ac.uk/software/bugs/}{BUGS} software change the world?}\spc{4}

\easysubproblem{Lindley said that Bayesian Statistics would win out over Frequentist Statistics because it was more logical. What in reality was the reason for the eventual victory of Bayes?}\spc{4}

\extracreditsubproblem{One of my PhD advisors, \href{https://statistics.wharton.upenn.edu/profile/563/}{Ed George} at Wharton told me that \qu{Bayesian Statistics is really `knowledge engineering.'} Is this true? Explain.}\spc{3}

\extracreditsubproblem{Take a look at the software \href{http://mc-stan.org/}{Stan}. What kind of potential does it have to change the world? Note: I had an opportunity to work on Stan as a postdoc (right after I finished his PhD) but chose to come to QC instead.}\spc{10}

\end{enumerate}







\problem{Now we will move to the Bayesian normal-normal model for estimating both the mean and variance and demonstrate similarities with the classical results.}

\begin{enumerate}

\intermediatesubproblem{If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and $X$ represents all $\Xoneton$, Find the kernel of $\cprob{\theta,~\sigsq}{X}$ if $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. Use the substitution that we made in class:

\beqn
\sum_{i=1}^n (x_i - \theta)^2 = (n-1)s^2 + n(\xbar - \theta)^2
\eeqn

where $s^2 := \oneover{n-1} \sum_{i=1}^n (x_i -\xbar)^2$. We do this here because this substitution is important for what comes next.}\spc{8}


\intermediatesubproblem{Using Bayes Rule, break up $\cprob{\theta,~\sigsq}{X}$ into two pieces. How are those two pieces distributed?}\spc{6}

\intermediatesubproblem{Using your answer from (b), explain in English how you can create samples from the distribution $\cprob{\theta,~\sigsq}{X}$ that look like $\braces{\bracks{\theta_1, \sigsq_1}, \bracks{\theta_2, \sigsq_2}, \ldots, \bracks{\theta_S, \sigsq_S}}$.}\spc{8}

\hardsubproblem{Using these samples, how would you estimate $\cexpe{\theta}{X}$ and $\cexpe{\sigsq}{X}$? Why is $\cexpe{\theta}{X}$ of paramount importance?}\spc{5}


\hardsubproblem{Using these samples, how would you estimate a 95\% CR for $\theta$?}\spc{5}

\hardsubproblem{Using these samples, how would you obtain a $p$-val for testing if $\sigsq > 1.364$?}\spc{5}

\hardsubproblem{[MA] Using these samples, how would you estimate $\corr{\theta~|~X}{~\sigsq~|~X}$ i.e. the correlation between the posterior distributions of the two parameters?}\spc{3}

%\easysubproblem{If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and $\theta \sim \normnot{\mu_0}{\tausq}$ write the distribution of $\theta~|~X,\sigsq$. Hint: it's in the notes and it was HW6 6(d). Note this problem is independent of the other problems.}\spc{5}

\easysubproblem{Find $\cprob{\theta}{X,~\sigsq}$ by using the full posterior kernel from (a) and then conditioning on $\sigsq$. You should get the same answer as we did before the midterm.}\spc{3}

\easysubproblem{Find $\cprob{\sigsq}{X,~\theta}$ by using the full posterior kernel from (a)  and then conditioning on $\theta$. You should get the same answer as we did before the midterm.}\spc{4}

%\intermediatesubproblem{Show that $\cprob{\sigsq}{X}$ is an inverse gamma distribution and find its parameters.}\spc{4}

\hardsubproblem{Show that $\cprob{\theta}{X}$ is a non-standard $T$ distribution and find its parameters. Assume the prior $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. The answer is in the notes, but try to do it yourself.}\spc{15}


\hardsubproblem{Show that $\cprob{\sigsq}{X}$ is an inverse gamma and find its parameters. Assume the prior $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. The answer is in the notes, but try to do it yourself.}\spc{12}


%\intermediatesubproblem{How does this compare to 2(j)? Note that $X \sim \invgammanot{\alpha}{\beta}$ then $cX \sim \invgammanot{\alpha}{\frac{\beta}{c}}$.}\spc{2}

\easysubproblem{Write down the distribution of $\cprob{X^*}{X}$ assuming the prior $\prob{\theta,~\sigsq} \propto \oneover{\sigsq}$. This is in the notes.}\spc{1}

\extracreditsubproblem{Prove what you wrote in the previous question: $\cprob{X^*}{X}$ is the non-standard $T$ distribution and find its parameters.}\spc{0}


\intermediatesubproblem{Explain how to sample from the distribution of $\cprob{X^*}{X}$. Also in the notes.}\spc{9}

\intermediatesubproblem{Now consider the informative conjugate prior of $\prob{\theta,~\sigsq} = \cprob{\theta}{\sigsq} \prob{\sigsq}$ where $\cprob{\theta}{\sigsq} = \normnot{\mu_0}{\frac{\sigsq}{m}}$ and $\prob{\sigsq} = \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$ i.e. the general normal-inverse-gamma. What is its kernel? Collect common terms and be neat.}\spc{9}


\hardsubproblem{[MA] If $\Xoneton~|~\theta, \sigsq \iid \normnot{\theta}{\sigsq}$ and given the general prior above, find the posterior and demonstrate it that the normal-inverse gamma is conjugate for the normal likelihood with both mean and variance unknown. This is what I did \emph{not} do in class.}\spc{13}

\end{enumerate}

\problem{We model the returns of S\&P 500 here.}

\begin{enumerate}
\easysubproblem{Below are the 16,428 daily returns (as a percentage) of the S\&P 500 dating back to January 4, 1950 and the code used to generate it. Does the data look normal? Yes/no}\spc{0}

\begin{figure}[h]
\centering
\includegraphics[width=7in]{daily_returns}
\end{figure}

%\begin{verbatim}
%X = read.csv('sp_tot_ret_price_1950.csv')
%n = nrow(X)
%n
%hist(X[,4], br = 1000, 
%  main = 'daily returns (as a percentage) of the S&P 500')
%\end{verbatim}

\intermediatesubproblem{Do you think the data is $\iid$? Explain.}\spc{1}

\intermediatesubproblem{Assume $\iid$ normal data regardless of what you wrote in (a) and (b). The sample average is $\xbar = 0.0003415$ and the sample standard deviation is $s = 0.0096$. Under an objective prior, give a 95\% credible region for the true mean daily return.}\spc{4}

\hardsubproblem{Give a 95\% credible region for \emph{tomorrow's} return using functions in Table~\ref{tab:eqs}.}\spc{4}

\end{enumerate}


\end{document}











